# -*- coding: utf-8 -*-import osimport tensorflow as tfimport inferenceimport numpy as npimport pdbimport matplotlib.pyplot as pltfrom tensorflow.examples.tutorials.mnist import input_dataBATCH_SIZE = 100LEARNING_RATE_BASE = 0.8LEARNING_RATE_DECAY = 0.99REGULARAZTION_RATE = 0.0001TRAINING_STEPS = 40000MOVING_AVERAGE_DECAY = 0.99MODEL_SAVE_PATH = "/home/zhangtx/ml/lintcodeProject/model"MODEL_NAME = "model.ckpt"DEFAULT_LABEL = [0]DEFAULT_FEATURE = [0.0]RECORD_NUM = 42000FILENAME = "./data/train.csv"TESTFILENAME = "./data/test.csv"#读取函数定义def data_transform(a):    print a    b = np.zeros([28, 28])    for i in range(0, 27):        for j in range(0, 27):            b[i][j] = a[28*i + j]            # print a[28*i + j]    return bdef read_data(file_queue):    reader = tf.TextLineReader(skip_header_lines=1)    key, value = reader.read(file_queue)    defaults = [DEFAULT_FEATURE for i in range(0, inference.INPUT_NODE)]    defaults.insert(0, DEFAULT_LABEL)    train_item = tf.decode_csv(value, defaults)    # pdb.set_trace()    feature = tf.multiply(1.0/255.0, train_item[1:])    label = train_item[0:1]    return feature, labeldef create_pipeline(filename, batch_size, num_epochs=None):    file_queue = tf.train.string_input_producer([filename], shuffle=True, num_epochs=num_epochs)    example, label = read_data(file_queue)    min_after_dequeue = 100    capacity = min_after_dequeue + 3*batch_size    example_batch, label_batch = tf.train.shuffle_batch(        [example, label], batch_size=batch_size, capacity=capacity,        min_after_dequeue=min_after_dequeue, num_threads=1    )    return example_batch, label_batchdef oneShot(label_batch):    num_labels = label_batch.shape[0]    index_offset = np.arange(num_labels) * 10    num_labels_hot = np.zeros((num_labels, 10))    num_labels_hot.flat[index_offset+label_batch.ravel()] = 1.0    return num_labels_hotdef train():    with tf.name_scope("input"):        x = tf.placeholder(tf.float32, [None, inference.INPUT_NODE], name="x-input")        y_ = tf.placeholder(tf.float32, [None, inference.OUTPUT_NODE], name="y-input")    regularizer = tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE)    y = inference.inference(x, regularizer)    global_step = tf.Variable(0, trainable=False)    with tf.name_scope("moving_average"):        variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)        variable_averages_op = variable_averages.apply(tf.trainable_variables())    with tf.name_scope("loss_function"):        cross_entroy = tf.nn.sparse_softmax_cross_entropy_with_logits(            logits=y, labels=tf.argmax(y_, 1))        cross_entroy_mean = tf.reduce_mean(cross_entroy)        loss = cross_entroy_mean + tf.add_n(tf.get_collection('losses'))        tf.summary.scalar("loss",loss)    with tf.name_scope("train_step"):        learning_rate = tf.train.exponential_decay(            LEARNING_RATE_BASE,            global_step,            RECORD_NUM/BATCH_SIZE,            LEARNING_RATE_DECAY)        train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)    with tf.control_dependencies([train_step, variable_averages_op]):        train_op = tf.no_op(name='train')    saver = tf.train.Saver()    writer = tf.summary.FileWriter("./logs/", tf.get_default_graph())    xs, ys = create_pipeline(FILENAME, BATCH_SIZE,num_epochs=1000)    init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())    sess = tf.Session()    coord = tf.train.Coordinator()    sess.run(init)    threads = tf.train.start_queue_runners(sess=sess, coord=coord)    merged_summary = tf.summary.merge_all()    try:        print("Training: 1")        count = 0        while not coord.should_stop():            xs_batch, ys_batch = sess.run([xs, ys])            # xs, ys = minst.train.next_batch(BATCH_SIZE)            # pdb.set_trace()            # print curr_x_train_batch            if count % 100 == 0:                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)                run_metadata = tf.RunMetadata()                _, loss_value, step, summary = sess.run([train_op, loss, global_step, merged_summary],                                               feed_dict={x: xs_batch, y_: oneShot(ys_batch)},                                               options=run_options,                                               run_metadata=run_metadata)                #writer.add_run_metadata(run_metadata, 'step%03d'% count)                print("After %d training steps,loss on training batch is %g." % (step, loss_value))                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)            else:                _, loss_value, step, summary = sess.run([train_op, loss, global_step, merged_summary],                                               feed_dict={x: xs_batch, y_: oneShot(ys_batch)})            writer.add_summary(summary, step)            count = count + 1            #print("After %s training steps,validation accuracy =%g" % (count, 0))    except tf.errors.OutOfRangeError:        print('Done training -- epoch limit reached')    finally:        # When done, ask the threads to stop.        coord.request_stop()    coord.join(threads)    sess.close()    writer.close()def main(argv=None):    train()    #    # mnist = input_data.read_data_sets("/home/zhangtx/ml/mnist/ministdata",one_hot=True)    # train(mnist)if __name__ == '__main__':    tf.app.run()